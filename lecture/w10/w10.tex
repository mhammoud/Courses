\documentclass[11pt]{article}
\input{/Users/markwang/.preamble}
\begin{document}

\section*{Simple Linear Regression}


\begin{defn*}
  \textbf{Method of Least Squares} is a method for determining parameters in curve-fitting problems, where we want to predict dependent variable $Y$ from $X$. Consider fitting simpliest model to data
  \[
    Y = \beta_0 + \beta_1 X
  \]
  Denote $i$th residual as
  \[
    e_i = y_i - \beta_0 - \beta_1 x_i = y_i - \hat{y}_i
  \]
  We want to minimize $e_i$ as small as possible. The \textbf{least squares estimators} of $\beta_0$ and $\beta_1$ are the minimizers of the \textbf{residual sum of squares}
  \[
    RSS(\beta_0, \beta_1) := \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2
   \]
   In other words we choose a linear fit $\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X$ such that
   \[
    (\hat{\beta}_0 ,\hat{\beta}_1) = \argmin_{\beta_0, \beta_1} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2
   \]
\end{defn*}

\begin{proposition*}
  The least squares estimators of $\beta_0$ and $\beta_1$ are given by
  \[
    \begin{cases}
      \hat{\beta}_1 = \frac{S_{XY}}{S_X^2}\\
      \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}
    \end{cases}
  \]
  where $S_{XY} = \frac{1}{n-1} \sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})$ is the \textbf{sample covariance} of $X$ and $Y$ and $S_X^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i - \overline{x})^2$ is the \textbf{sample variance} of $X$.
\end{proposition*}

\begin{lemma*}
  \textbf{some useful properties in proving previous proposition and facilitates computation}
  \begin{enumerate}
    \item
    \[
      \sum_{i=1}^n (x_i - \overline{x})^2 = \sum_{i=1}^n x_i^2 - 2n\overline{x}^2 + n\overline{x}^2= \sum_{i=1}^n x_i^2 - n\overline{x}^2
    \]
    \item
    \[
      \sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y}) = \sum_{i=1}^n x_iy_i - 2n\overline{x}\overline{y} + n\overline{x}\overline{y} =\sum_{i=1}^n x_iy_i - n\overline{x}\overline{y}
    \]
  \end{enumerate}
\end{lemma*}

\begin{defn*}
  \textbf{The normal equations} Denote $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i$, $i = 1, \cdots, n$ the residue is hence $e_i = y_i -\hat{y}_i$. The residuals of the least square fit satisfy
  \begin{enumerate}
    \item
    \[
      0 = \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = \sum_{i=1}^n (y_i - \hat{y}_i) = \sum_{i=1}^n e_i
    \]
    \item
    \[
      0 = \sum_{i=1}^n x_i (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i) = \sum_{i=1}^n x_i (y_i - \hat{y}_i) = \sum_{i=1}^n x_i e_i
    \]
  \end{enumerate}
  which is derived from the process of finding least squared estimators, when we are taking first order partial with respect to $\beta_0$ and $\beta_1$
\end{defn*}

\begin{defn*}
  \textbf{Standard Statistical Model} stipulates that the observed value of $y$ is a linear function of $x$ plus random noise
  \[
    y(x) = \beta_0 + \beta_1 x + \epsilon(x)
  \]
  where $x$ is not a random variable, and $y(x)$ is a random through inclusion of random noise $\epsilon(x)$ where we assume
  \begin{enumerate}
    \item by Normal equation
    \[
      \E(\epsilon(x)) = 0 \text{ for all } x
    \]
    \item Noise at different $x$ are uncorrelated and variance around the regression line is same for all values of $x$ (homoscedasticity)
    \[
      Cov(\epsilon(x), \epsilon(x')) =
      \begin{cases}
        \sigma^2 & x = x' \\
        0 & x \neq x' \\
      \end{cases}
    \]
  \end{enumerate}
  Hence the model can be denoted as
  \[
    y_i = \beta_0 + \beta_1 x_i + \epsilon_i, \quad i = 1, \cdots, n
  \]
  with
  \[
    \E[\epsilon_i] = 0 \text{ for all } i  \quad \quad Var(\epsilon_i) = \sigma^2 \text{ for all } i \quad \quad \E[\epsilon_i \epsilon_j] = 0 \text{ for } i\neq j
  \]
  \[
    hence \quad Var[y_i] = 0 \quad \quad Cov(y_i, y_j) = 0 \text{ for } i\neq j
  \]
\end{defn*}

\begin{defn*}
  \textbf{LS estimator as linear estimator} Let $y_1, \cdots, y_n\sim f_{\theta}$. Any estimator of $\theta$ of the form
  \[
    \hat{\theta} = \sum_{i=1}^n c_i y_i
  \]
  is called a linear estimator (linear combination of observations)

  \begin{enumerate}
    \item $\hat{\beta}_0$ and $\hat{\beta}_1$ are linear estimators.
    \[
      \hat{\beta}_1 = \frac{\sum_i (x_i - \overline{x})y_i}{\sum_j (x_j - \overline{x})^2} = \sum_i a_i y_i \quad\quad\text{ where }\quad\quad a_i = \frac{x_i - \overline{x}}{\sum_j (x_j - \overline{x})^2}
    \]
    \[
      \hat{\beta}_0 = \frac{1}{n} \sum_i y_i - \overline{x} \frac{\sum_i (x_i - \verline{x})y_i}{\sum_j (x_j - \overline{x})^2} = \sum_i b_i y_i \quad\quad \text{ where }\quad \quad b_i = \frac{1}{n} - \frac{ \overline{x}(x_i - \verline{x})}{\sum_j (x_j - \overline{x})^2}
    \]
    \item In fact, they are also unbiased estimators, i.e. $\E[\hat{\beta}_0] = \beta_0$ and $\E[\hat{\beta}_1] = \beta_1$
    \item
    \[
      Var[\hat{\beta}_1] = Var\left[\sum_i a_i y_i\right] = \sigma^2 \sum_i a_i^2 = = \frac{\sigma^2}{\sum_i (x_i - \overline{x} )^2}
    \]
    \[
      Var[\hat{\beta}_0] = \sigma^2 \left[ \frac{1}{n} + \frac{\overline{x}^2}{\sum_i (x_i - \overline{x})^2} \right]
      \quad \quad \text{ and }\quad \quad
      Cov(\hat{\beta}_0, \hat{\beta}_1) = -\frac{\sigma^2 \overline{x}}{ \sum_i (x_i - \overline{x})^2}
    \]
    \item Unbiased estimator of noise variance is given by
    \[
      S^2 = \frac{1}{n - 2} \sum_{i=1}^n e_i^2
    \]
  \end{enumerate}
\end{defn*}



\begin{defn*}
  \textbf{The Gauss-Markov Theorem} Under standard model assumptions, no linear unbiased estimator of $\beta_0$ ($\beta_1$) has a smaller variance than the least squares estimator $\hat{\beta}_0$ ($\hat{\beta}_1$).
  \begin{rem}
    This shows that least square estimators are the \textbf{best linear unbiased estimator (BLUE)}
  \end{rem}
\end{defn*}



\begin{defn*}
  \textbf{Correlation Coefficient} The correlation coefficient of random variables $X$ and $Y$ is
  \[
    \rho_{XY} = \frac{Cov(X, Y)}{\sigma_X \sigma_Y}
  \]
  where $\sigma_X$ and $\sigma_Y$ are the standard deviations of $X$ and $Y$, respectively.
  \begin{enumerate}
    \item $|\rho_{XY}| \leq 1$, where equality holds iff $X$ and $Y$ are perfect linear function of one another. In other words, $|\rho_{XY}|$ is a measure of linear relationship between $X$ and $Y$
    \item \textbf{Sample Correlation Coefficient} is defined to be
    \[
      r_{XY} = \frac{S_{XY}}{S_X S_Y} = \frac{\sum_i (x_i - \overline{x}) (y_i - \overline{y}) }{ \sqrt{ \sum_i (x_i - \overline{x})^2  }  \sqrt{ \sum_i (y_i - \overline{y})^2  } }
    \]
    which shares the above property and happens to be a consistent estimator of $\rho_{XY}$
  \end{enumerate}
\end{defn*}

\begin{defn*}
  \textbf{Explained Variation} Variation in value of $Y$ is the \textbf{Total Sum of Squares}
  \[
    TSS = \sum_{i=1}^n (y_i - \overline{y})^2
  \]
  Now
  \begin{align*}
    \sum_{i=1}^n (y_i - \overline{y})^2 &= \sum_{i=1}^n (y_i - \hat{y}_i)^2 + \sum_{i=1}^n (\hat{y}_i - \overline{y})^2 + 2\sum_{i=1}^n (y_i - \hat{y}_i)(\hat{y}_i - \overline{y}) \\
    &= \sum_{i=1}^n (y_i - \hat{y}_i)^2 + \sum_{i=1}^n (\hat{y}_i - \overline{y})^2 \tag{by Normal Equation}\\
    TSS &= RSS + ESS\\
  \end{align*}
  the \textbf{Proportion of explained variance} is defined to be
  \[
    R^2 = \frac{ESS}{TSS} = \frac{ \sum_{i=1}^n (\hat{y}_i - \overline{y})^2 }{ \sum_{i=1}^n (y_i - \overline{y})^2} \quad \quad \text{ and }  \quad\quad  R^2 = r_{XY}^2
  \]
  $R^2$ is an indication of good linear fit
\end{defn*}


\subsection*{Statistical Inference under Gaussian Noise}

\begin{defn*}
  A linear model with following assumptions
  \begin{enumerate}
    \item $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$ where $i = 1, \cdots, n$
    \item $\E[\epsilon_i] = 0$ for $i = 1,\cdots,n$
    \item $Var(\epsilon_i) = \sigma^2$ $i = 1,\cdots, n$ (homoscedastic) and $\E[\epsilon_i \epsilon_j] = 0$ for $i\neq j$ (uncorrelated)
    \item distribution of $\epsilon_i$  is normal for $i = 1,\cdots,n$
    \item Uncorrelated normal random variable is independent.
  \end{enumerate}
  allows for \textbf{statistical inference}, i.e. hypothesis testing, calculate confidence interval etc. An unbiased estimator of noise variance $\sigma^2$ is given by
  \[
    S^2 = \frac{1}{n - 2} \sum_{i=1}^n e_i^2 \quad \quad \text{ and } \quad \frac{(n-2)S^2}{\sigma^2} \sim \chi_{n-2}^2
  \]
  Since $\epsilon_i$ is normal, we have
  \[
    y_i \sim \mathcal{N}(\beta_0 + \beta_1 x_i, \sigma^2)
  \]
  Since $\hat{\beta}_0$ and $\hat{\beta}_1$ are linear estimators, i.e. of the form $\hat{\beta} = \sum_i c_i y_i$ , then we derive \textbf{Regression coefficients}
  \[
    \hat{\beta}_1 \sim \mathcal{N}(\beta_1, \frac{\sigma^2}{\sum_i (x_i - \overline{x})^2})
  \]
  where $\sigma^2$ is variance of error. Under normality
  \[
    \frac{ \hat{\beta}_1 - \beta_1 }{ \frac{\sigma}{\sqrt{ \sum_i (x_i - \overline{x})^2 }}} \sim \mathcal{N}(0,1)
  \]
  Now we replace unknown $\sigma^2$ with unbiased estimator $S^2 = \frac{1}{n-2}\sum_i e_i^2$ we have
  \[
    \frac{ \hat{\beta}_1 - \beta_1 }{ \frac{S}{\sqrt{ \sum_i (x_i - \overline{x})^2 }}} \sim t_{n-2}
  \]
  We can then do hypothesis tests on the slope coefficient $\hat{\beta}_1$
\end{defn*}

\begin{defn*}
  \textbf{Hypothesis tests on the slope $\beta_1$ to evaluate correlation} Testing $\mathcal{H}_0: \beta_1 = 0$ vs. $\mathacl{H}_1: \beta_1 \neq 0$, then, by
  \[
    \mathcal{T} =
    \frac{\hat{\beta}_1 - \beta_1}{s_{\hat{\beta}_1}} \stackrel{\mathacl{H}_0}{\sim} t_{n-2} \quad\quad \text{ where }\quad\quad s_{\hat{\beta}_1} = \sqrt{\frac{\frac{1}{n-2} \sum_i e_i^2}{\sum_i (x_i - \overline{x})^2}}
  \]
  and a $100(1-\alpha)\%$ confidence interval for $\beta_1$ is given by
  \[
    \hat{\beta}_1 \pm t_{n-2, 1 - \alpha/2} \frac{S}{\sqrt{\sum_i (x_i - \overline{x})^2 }}
  \]
\end{defn*}


\begin{defn*}
  \textbf{Confidence interval for mean response} Want to estimate mean response
  \[
    \mu(x_0) := \E[y(x_0)] = \beta_0 + \beta_1 x_0
  \]
  The prediction at $x_0$ may be used as an estimator
  \[
    \hat{y}(x_0) = \hat{\beta}_0 + \hat{\beta}_1 x_0
  \]
  we have
  \begin{align*}
    \E[\hat{y}(x_0)] = \E[\hat{\beta}_0 + \hat{\beta}_1 x_0] = \E[\hat{\beta}_0] + \E[\hat{\beta}_1] x_0 = \beta_0 + \beta_1 x_0  = \mu(x_0)\\
    Var[\hat{y}(x_0)] = Var[\hat{\beta}_0 + \hat{\beta}_1 x_0] = \sigma^2 \left\{ \frac{1}{n} + \frac{(x_0 - \overline{x})^2}{\sum_i (x_i - \overline{x})^2}  \right\}
  \end{align*}
  Since least square estimators are linear estimators we can write
  \[
    \hat{y}(x_0) = \hat{\beta}_0 + \hat{\beta}_1 x_0 = \sum_i b_i y_i + x_0 \sum_i a_i y_i = \sum_i (b_i + x_0 a_i)y_i
  \]
  hence $\hat{y}(x_0)$ has normal distribution so then,
  \[
    \hat{y}(x_0) \sim \mathcal{N} \left( \mu(x_0), \sigma^2 \left\{ \frac{1}{n} + \frac{(x_0 - \overline{x})^2}{\sum_i (x_i - \overline{x})^2}  \right\}  \right)
  \]
  Now we replace $\sigma^2$ with $S^2 = \frac{1}{n-2}\sum_i e_i^2$ would result in $t$ distribution
  \[
    \hat{y}(x_0) \pm t_{n-2, 1-\alpha/2} S \sqrt{ \frac{1}{n} + \frac{(x_0 - \overline{x})^2}{\sum_i (x_i - \overline{x})^2} }
  \]
  is a $100(1-\alpha)\%$ confidence interval for mean response $\E[y(x_0)]$, where $\hat{y}(x_0) = \hat{\beta}_0 + \hat{\beta}_1 x_0$. As $x_0$ vary, we have a confidence band centered at the least squares fit, that get narrower as $x_0$ draws closer to $\overline{x}$
\end{defn*}

\begin{defn*}
  \textbf{Least Square Estimators under standard normal model are MLEs} Under additional assumption that random noise is Gaussian, the least squares estimators $\hat{\beta}_0^{LS}$ and $\hat{\beta}_1^{LS}$ are maxmum likelihood estimators of $\beta_0$ and $\beta_1$, respectively.
\end{defn*}

\subsection*{Diagnostic Plot}

\begin{defn*}
  Linear regressio model relies heavily on assumptions about random errors $\epsilon_i$. the residual $e_i$ should be
  \begin{enumerate}
    \item normal
    \item independent
    \item homoscedasticitic
    \item distribution of \textbf{standardized residuals} $\frac{e_i}{S} \sim \mathcal{N}(0,1)$
  \end{enumerate}
  Two plots are given
  \begin{enumerate}
    \item \textbf{Residuals vs Fitted Value Plot} plot of $e_i$ vs. $\hat{y}_i$.
      \begin{enumerate}
        \item Symmetry about 0, with homogeneity of the noise variance (homoscedastic), and no trends or pattern implies a good fit for linear models
        \item Streaks of positive/negative residual indicates observation is correlated, violating the independence assumption
        \item The trend resembles an upward or downward curve indicates model misspecification. The assumption of linearity is violated
        \item Increasing variance along the dependent $\hat{y}_i$ axis violates homoscedasticity assumption
      \end{enumerate}
    \item \textbf{Quantile-Quantile Plot} A plot for comparing two probability distributions by plotting their quantiles against each other. In evaluating good fit for linear model we plot sample quantiles of standardized residues vs. theoretical quantiles of standard normal distribution.
    \begin{enumerate}
      \item If points approximately lie on the line $y=x$, then the distribution in comparison are similar, i.e. $\frac{e_i}{S} \sim \mathcal{N}(0,1)$.
      \item If lower quantiles are too small and upper quantiles are too large - a heavy-tailed noise. Perhaps assuming t distribution, thus violating the normality assumption
    \end{enumerate}
  \end{enumerate}
\end{defn*}



\end{document}
